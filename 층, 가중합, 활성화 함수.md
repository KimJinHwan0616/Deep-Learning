># 딥러닝
>`심층 신경망`*(DNN)* 모델 알고리즘
>### 가중합, 활성화 함수, 층
>### 순전파, 역전파
```
순전파 → 손실&오차 → 역전파 → 순전파 → 손실&오차 → 역전파 → ...
```
---

## 가중합 *(ｚ)*
가중치1 ｘ 특성1 + ... + 가중치n ｘ 특성n + 편향

+ ### 가중치 *(weight, 중요도)*
  `입력값` 크기 조정

---

## 활성화 함수 *(Activation Function)*
ｚ → 출력값

+ ### 출력층 *(분류)*
  `시그모이드`, `소프트맥스`

+ ### 은닉층 *(피드포워드 신경망)*
  `렐루` *max(0, ｚ)*, `리키 렐루`

+ ### 은닉층 *(순환 신경망)*
  `하이퍼볼릭 탄젠트` *tanh²*

---

## 층 *(layer)*
뉴런 = 노드

+ ### 입력층 *(input layer)*
  `특성` 노드 집합

+ ### 은닉층 *(hidden layer)*
  노드(`가중합` + `활성화 함수`) 집합
  ```
  입력값 → 은닉층 → 출력값
  ```
  
+ ### 출력층 *(output layer)*
  `ｚ` 노드 집합

---

## 순전파 *(feedforward)*
입력층 → 은닉층 → 출력층

## 역전파 *(backpropagation)*
출력층 → 은닉층 → 입력층

+ ### 기울기 소멸 *(Gradient Vanishing)*
  입력층에 가까운 층들에서 가중치가 업데이트 되지 않는 현상
  ```
  → 렐루 함수 사용 
  ```
  
+ ### 과적합 *(overfitting)*
  훈련 데이터 정확도 >> 테스트 데이터 정확도
  
  >드롭 아웃 *(dropout)*
  >```
  >모델 훈련: 임의의 노드 제외O (출력값=0)
  >모델 예측: 노드 제외X
  >``` 
  >옵티마이저 *(optimizer)*: `확률적 경사 하강법` 개선
  >```
  >- 속도
  >  모멘텀, 네스테로프 모멘텀
  >- 운동량
  >  아다그라드, 아다델타, 알엠에스프롭
  >- 속도 + 운동량
  >  아담(알엠에스프롭 + 모멘텀)
  >```

